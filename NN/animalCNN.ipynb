{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomsmail/Documents/Hackathons/DurHack2024/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /Users/tomsmail/.cache/kagglehub/datasets/iamsouravbanerjee/animal-image-dataset-90-different-animals/versions/5/animals/animals/\n"
     ]
    }
   ],
   "source": [
    "# A CNN using torch to classify animals \n",
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "# You need a Kaggle API key to download datasets (ask Tom)\n",
    "path = kagglehub.dataset_download(\"iamsouravbanerjee/animal-image-dataset-90-different-animals\")\n",
    "\n",
    "# Path to dataset files not properly configured, add to fix\n",
    "path = path + \"/animals/animals/\"\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.models import mobilenet_v2\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'antelope': 0, 'badger': 1, 'bat': 2, 'bear': 3, 'bee': 4, 'beetle': 5, 'bison': 6, 'boar': 7, 'butterfly': 8, 'cat': 9, 'caterpillar': 10, 'chimpanzee': 11, 'cockroach': 12, 'cow': 13, 'coyote': 14, 'crab': 15, 'crow': 16, 'deer': 17, 'dog': 18, 'dolphin': 19, 'donkey': 20, 'dragonfly': 21, 'duck': 22, 'eagle': 23, 'elephant': 24, 'flamingo': 25, 'fly': 26, 'fox': 27, 'goat': 28, 'goldfish': 29, 'goose': 30, 'gorilla': 31, 'grasshopper': 32, 'hamster': 33, 'hare': 34, 'hedgehog': 35, 'hippopotamus': 36, 'hornbill': 37, 'horse': 38, 'hummingbird': 39, 'hyena': 40, 'jellyfish': 41, 'kangaroo': 42, 'koala': 43, 'ladybugs': 44, 'leopard': 45, 'lion': 46, 'lizard': 47, 'lobster': 48, 'mosquito': 49, 'moth': 50, 'mouse': 51, 'octopus': 52, 'okapi': 53, 'orangutan': 54, 'otter': 55, 'owl': 56, 'ox': 57, 'oyster': 58, 'panda': 59, 'parrot': 60, 'pelecaniformes': 61, 'penguin': 62, 'pig': 63, 'pigeon': 64, 'porcupine': 65, 'possum': 66, 'raccoon': 67, 'rat': 68, 'reindeer': 69, 'rhinoceros': 70, 'sandpiper': 71, 'seahorse': 72, 'seal': 73, 'shark': 74, 'sheep': 75, 'snake': 76, 'sparrow': 77, 'squid': 78, 'squirrel': 79, 'starfish': 80, 'swan': 81, 'tiger': 82, 'turkey': 83, 'turtle': 84, 'whale': 85, 'wolf': 86, 'wombat': 87, 'woodpecker': 88, 'zebra': 89}\n",
      "Number of train batches: 135\n",
      "Number of validation batches: 17\n",
      "Number of test batches: 17\n"
     ]
    }
   ],
   "source": [
    "# Define transformations for the training data and testing data\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Load the dataset\n",
    "dataset = torchvision.datasets.ImageFolder(root=path, transform=transform)\n",
    "\n",
    "# Split the dataset into training, validation and  testing sets\n",
    "train_size, val_size, test_size = int(0.8 * len(dataset)), int(0.1 * len(dataset)), int(0.1 * len(dataset))\n",
    "train_set, val_set, test_set = torch.utils.data.random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "# Create a DataLoader\n",
    "train_loader = DataLoader(train_set, batch_size=32, shuffle=True)\n",
    "\n",
    "print(dataset.class_to_idx)\n",
    "# for i, (batch, batch_targets) in enumerate(train_loader):\n",
    "#     print(batch_targets)\n",
    "\n",
    "\n",
    "# Don't shuffle the validation and testing data - no point\n",
    "val_loader = DataLoader(val_set, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_set, batch_size=32, shuffle=False)\n",
    "\n",
    "# Print the number of batches\n",
    "print(f\"Number of train batches: {len(train_loader)}\")\n",
    "print(f\"Number of validation batches: {len(val_loader)}\")\n",
    "print(f\"Number of test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomsmail/Documents/Hackathons/DurHack2024/venv/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/tomsmail/Documents/Hackathons/DurHack2024/venv/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 2.5972\n",
      "Epoch [2/5], Loss: 1.7032\n",
      "Epoch [3/5], Loss: 0.9821\n",
      "Epoch [4/5], Loss: 0.6132\n"
     ]
    }
   ],
   "source": [
    "# Load the pre-trained MobileNet model\n",
    "model = mobilenet_v2(pretrained=True)\n",
    "\n",
    "# Freeze all the layers\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "num_classes = 90\n",
    "model.classifier[1] = nn.Linear(model.classifier[1].in_features, num_classes)\n",
    "\n",
    "# Unfreeze the last few layers for fine-tuning\n",
    "for param in model.features[-5:].parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Print the model architecture to verify\n",
    "# print(model)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define the optimizer with momentum\n",
    "optimizer = optim.SGD(filter(lambda p: p.requires_grad, model.parameters()), lr=0.001, momentum=0.9)\n",
    "\n",
    "\n",
    "loss_values = []\n",
    "\n",
    "# Example training loop\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for images, labels in train_loader:\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_values.append(loss.item())\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "plt.plot(range(1, len(loss_values) + 1), loss_values, marker='o')\n",
    "plt.xlabel('Batch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Over Time')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([16, 11,  0, 65, 83,  8, 72, 11, 36, 44,  0, 81, 53, 84, 81, 75, 59, 85,\n",
      "        14, 77,  7, 13, 69, 28,  3,  0, 38, 41, 74, 16, 16, 48])\n",
      "tensor([65, 14, 29,  9, 65, 39, 71,  5, 82, 69, 19, 23, 48, 29, 60, 84, 42, 80,\n",
      "        27, 63,  0, 48,  0, 55, 58,  6, 68, 58,  8, 84, 29, 83])\n",
      "tensor([11, 77,  0, 29, 51, 34, 55, 49, 43,  8, 46, 48, 55, 40, 61, 45, 46, 69,\n",
      "        48, 70, 42, 36, 69, 16, 53, 11, 87,  5, 48, 15, 31, 11])\n",
      "tensor([ 4, 29, 23, 19, 31, 36, 60, 61, 60, 60, 23, 58, 59, 16, 85, 66, 45, 16,\n",
      "        65, 51, 53,  8, 69, 49, 58, 31, 67, 67, 42, 77, 60, 19])\n",
      "tensor([55, 43,  5, 55, 82, 31, 19, 24, 55, 80, 88, 81,  3, 32, 64, 29, 80, 19,\n",
      "        59, 60, 45, 49, 88, 60, 22, 46, 67, 52,  8, 81,  5, 74])\n",
      "tensor([72, 28, 55, 53, 49, 24, 34, 83,  6, 31, 19, 12, 83, 49, 11, 35, 24, 30,\n",
      "        13, 24, 24, 11,  4, 68, 27, 59, 44, 81, 44, 69, 63, 39])\n",
      "tensor([15, 23,  7, 43, 76,  0, 66, 34, 49, 27, 86, 53, 73,  8, 15,  0, 80, 30,\n",
      "        27, 45, 82,  8, 61, 23,  5, 68, 34, 88, 44, 38,  5, 76])\n",
      "tensor([25, 51, 32, 13, 55, 65, 68, 45, 56,  6, 85, 48, 25, 81, 49, 29, 29, 58,\n",
      "        29, 59, 15, 61,  4, 32,  1, 51, 44, 75, 19, 58, 72, 77])\n",
      "tensor([85, 28, 66, 56, 40, 43, 56, 62, 23, 68, 69, 53, 49, 84, 84, 39, 51, 14,\n",
      "        83, 12, 86, 23, 16, 89, 16,  8, 19, 83, 81, 18, 58, 77])\n",
      "tensor([53,  1, 12, 71, 45, 62, 53, 59, 63, 15, 67, 86, 69, 53, 54, 41, 35, 45,\n",
      "        21, 66, 65, 25, 24, 84, 12, 46, 81, 77, 49,  5, 59, 68])\n",
      "tensor([ 3, 55,  6,  3, 61, 61, 58, 49, 23, 74, 48, 29, 23,  6, 49, 53, 35, 44,\n",
      "        60, 65,  0, 74, 54, 65, 62, 59, 85, 83, 12, 30, 78,  4])\n",
      "tensor([58, 71,  9, 16, 84, 48, 82, 82,  3, 54, 40, 72, 76, 48, 48, 29, 49, 71,\n",
      "        66, 60, 47, 47, 60, 23, 18, 74, 54, 55, 31, 15,  7,  6])\n",
      "tensor([85, 16, 80, 81,  1, 54, 61, 53, 29, 45, 49, 89, 44, 85, 51, 70, 48, 11,\n",
      "        36, 27, 33, 70,  3,  6, 30, 58, 18, 31, 55, 48, 67, 52])\n",
      "tensor([14, 15, 26, 27, 14, 24,  3, 58, 31, 32, 46, 64, 71, 58, 80, 59, 54, 32,\n",
      "        36, 39, 44, 31, 53, 88, 16, 31, 21, 89, 53, 85, 49, 62])\n",
      "tensor([39, 44,  7, 29, 29,  3,  5, 84,  4, 25, 34, 85, 45, 72, 15, 16,  3, 18,\n",
      "        89, 50, 21, 52, 31, 81, 77, 58, 70, 74, 60,  8, 10, 38])\n",
      "tensor([33, 82, 66, 29, 29,  1, 69, 77, 50, 16, 78, 49,  3, 85,  1, 69, 15, 89,\n",
      "        44,  7, 53, 19, 63, 81, 53, 32, 29, 53,  6, 85, 83, 53])\n",
      "tensor([28, 65, 59, 85, 31, 82, 60, 59, 24,  4, 67, 69, 51,  0, 24, 53, 70, 58,\n",
      "        12, 52, 78, 70, 63,  4, 64, 62, 49,  4])\n",
      "Validation accuracy: 54.0741%\n"
     ]
    }
   ],
   "source": [
    "# Model evaluation\n",
    "model.eval()\n",
    "tp_tn_fp_fn = torch.zeros(4, num_classes)\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        print(predicted)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f\"Validation accuracy: {(100 * correct / total):.4f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model to disk\n",
    "torch.save(model.state_dict(), \"animal_model.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
